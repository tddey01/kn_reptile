requests模块
      - urllib模块
      - requests模块
          - requests模块：python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。
          - 作用: 模拟浏览器发请求

如何使用：
    - requests模块使用
        -  指定URL地址
        -  发起请求
        -  获取响应数据
        -  持久化存储
环境安装
    - pip3 install requests


实战编码环境
      -  请求搜狗首页页面数据

实战巩固
      - 需求  爬取搜狗指定词条对应搜索结果页面，（简易网页采集器）
           - UA检测
           - UA伪装
      - 需求  破解百度翻译
      - 需求  爬取豆瓣电影分类排行榜，https://movie.douban.com/中电影详情数据
      - 需求  爬取肯德基餐厅查询 https://www.kfc.com.cn/kfccda/index.aspx中指定地点的餐厅数据
      - 需求 爬取国家药品监督管理总局中基于中华人民共和国化妆品生产许可证相关数据 https://125.3.56.84:81/
         - 动态加载数据
         - 首页中对应的企业信息数据是通过ajax动态请求到的

         - 通过对详情页面URL的观察发现
             - url域名是一样的，只有携带的参数不一样
             - id值可以从首页对应的ajax请求到的json串中获取
             - 域名和idhi拼接处完整的一个企业对应详情页的url
        - 详情页面也是动态加载出来的

数据解析
       - 回顾requests模块实现数据爬取的流程
          - 指定url
          - 发起请求
          - 获取响应数据
          - 持久化存储
              - 聚焦爬虫：爬取页面中指定的页面内容
              - 其实，在上述流程中还需要较为重要的一步，就是在持久化存储之前需要进行指定数据解析。因为大多数情况下的需求，我们都会指定去使用聚焦爬虫，也就是爬取页面中指定部分的数据值，而不是整个页面的数据。因此，本次课程中会给大家详细介绍讲解三种聚焦爬虫中的数据解析方式。至此，我们的数据爬取的流程可以修改为：

              - 指定url
              - 发起请求
              - 获取响应数据
              - 数据解析
              - 持久化存储

python如何实现数据解析
      正则表达式
      xpath解析
      bs4解析

数据解析原理概念
      - 解析的局部的文件内容都会在标签之间或者标签对应属性中进行存储
          - 进行指定标签的定位，
          - 标签或者标签对应属性中存储的数据值进行提取（解析）
      - 正则表达式
           - 常用正则表达式回顾
               - 单字符:
                  .  : 除换行符以外所有字符
                  [] : [aoe] [a-w] 匹配集合中任意一个字符
                  \d : 数字 [0-9]
                  \D : 非数字
                  \w : 数字、字母、下划线、中文
                  \W : 非\W
                  \s : 所有的空白字符包、空格符、制表符、换页符等等。等价于 [\f\n\r\t\v]
                  \s : 非空白
               - 数量装饰；
                     *     :  任意多次 >=0
                     +     :  至少一次 >=1
                     ?     :  可有可无 0次或者1次
                     {m}   :  固定m次 hello{3,}
                     {m,}  :  至少m次
                     {m,n} :  m-n次
               - 边界
                     $ : 以某某结尾
                     ^ : 以某某开头
               - 分组
                     (ab)
               - 贪婪模式 : .*
               - 非贪婪（惰性) 模式 : .*?
               - re.I  : 忽略大小写
               - re.M  : 多行匹配
               - re.S  : 单行匹配
               - re.sub（正则表达式，替换内容，字符串）
      - 正则练习
            import re
            #提取出python
            key="javapythonc++php"
            re.findall('python',key)[0]
            #####################################################################
            #提取出hello world
            key="
            hello world
            "
            re.findall('
            (.*)
            ',key)[0]
            #####################################################################
            #提取170
            string = '我喜欢身高为170的女孩'
            re.findall('\d+',string)
            #####################################################################
            #提取出http://和https://
            key='http://www.baidu.com and https://boob.com'
            re.findall('https?://',key)
            #####################################################################
            #提取出hello
            key='lalalahellohahah' #输出hello
            re.findall('<[Hh][Tt][mM][lL]>(.*)',key)
            #####################################################################
            #提取出hit.
            key='bobo@hit.edu.com'#想要匹配到hit.
            re.findall('h.*?\.',key)
            #####################################################################
            #匹配sas和saas
            key='saas and sas and saaas'
            re.findall('sa{1,2}s',key)
      - 项目需求：爬取糗事百科指定页面的糗图，并将其保存到指定文件夹中
            #!/usr/bin/env python
            # -*- coding:utf-8 -*-
            import requests
            import re
            import os
            if __name__ == "__main__":
            url = 'https://www.qiushibaike.com/pic/%s/'
            headers={
               'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',
            }
            #指定起始也结束页码
            page_start = int(input('enter start page:'))
            page_end = int(input('enter end page:'))
            #创建文件夹
            if not os.path.exists('images'):
               os.mkdir('images')
            #循环解析且下载指定页码中的图片数据
            for page in range(page_start,page_end+1):
               print('正在下载第%d页图片'%page)
               new_url = format(url % page)
               response = requests.get(url=new_url,headers=headers)
               #解析response中的图片链接
               e = '
            .*?.*?'
               pa = re.compile(e,re.S)
               image_urls = pa.findall(response.text)
                #循环下载该页码下所有的图片数据
               for image_url in image_urls:
                   image_url = 'https:' + image_url
                   image_name = image_url.split('/')[-1]
                   image_path = 'images/'+image_name
                   image_data = requests.get(url=image_url,headers=headers).content
                   with open(image_path,'wb') as fp:
                       fp.write(image_data)
                       
